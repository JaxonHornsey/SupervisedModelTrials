<h1> Supervised Model Comparison </h1>
This repository contains the results of a project that compares four supervised machine learning models (K Nearest Neighbors, Decision Trees, Multi-layer Perceptron, and Support Vector Machine) on a classification dataset from the UC Irvine Machine Learning Repository. The dataset consists of 8124 instances and 22 attributes, and the goal of the tests is to determine the best prediction accuracy and speeds between the models. The repository also includes code to find the best hyperparameters for each model.

<h2>Methodology</h2>
The classification dataset for this project was imported from the UC Irvine Machine Learning Repository using Python in the Jupyter Notebook environment. We used the pandas' library to convert the dataset into a data frame and modified the string values to numerics in order to use them with the models. We then separated the classification results to predict.

To create a training set for each model to learn from, we used the train_test_split method from the scikit-learn library to split our data into random train and test subsets. We used these random subsets as our test sets for each model.

We ran identical tests for each model, recording the time and accuracy with bad parameters, and then again with the best parameters found using the grid search cross-validation method from scikit-learn. We also tried different test sets and averaged the results.

To create a statistical model to appropriately demonstrate the results, we used the matplotlib and seaborn libraries to create classification tables, confusion matrices, and other visualizations.

<h2> Results and Discussion </h2>
The results of the model comparison showed that the decision tree model had the highest accuracy at 98.13%, followed by k nearest neighbors at 95.7%, support vector machines at 94.7%, and multi-layer perceptron at 94.4%. Decision trees are a popular choice for classification tasks due to their simplicity and interpretability. They work by creating a tree-like model of decisions based on the features of the data, allowing them to make predictions based on the most relevant features. K nearest neighbors is a method that classifies a data point based on the majority class of its nearest neighbors. Support vector machines are a type of linear classifier that finds the hyperplane in a high-dimensional space that maximally separates the classes. Finally, multi-layer perceptrons are a type of neural network that can learn nonlinear relationships in the data. Overall, these results show that decision trees were the most effective model for this classification dataset, with the other models performing somewhat less well.

<br></br>
In conclusion, this project demonstrated the effectiveness of decision trees for classification tasks on the UC Irvine mushroom classification dataset. While the other models tested (k nearest neighbors, support vector machines, and multi-layer perceptron) also had relatively high accuracy scores, the decision tree model outperformed them by a significant margin. These results highlight the importance of considering multiple models and carefully tuning their hyperparameters in order to achieve the best performance on a given dataset. Overall, this project provided valuable insights into the strengths and limitations of different supervised machine learning models and the importance of model selection and hyperparameter tuning, specifically for mushroom classification tasks.
